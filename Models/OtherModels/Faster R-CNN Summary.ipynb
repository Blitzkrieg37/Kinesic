{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RPN(Region Proposal Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "To generate region proposals, we slide a small network over the conv feature map output by the last\n",
    "shared conv layer. This network is fully connected to an n × n spatial window of the input conv feature map. \n",
    "Each sliding window is mapped to a lower-dimensional vector (256-d for ZF and 512-dfor VGG). \n",
    "This vector is fed into two sibling fully-connected layers—a box-regression layer (reg)\n",
    "and a box-classification layer (cls).\n",
    "\n",
    "This architecture is naturally implemented with an **n × n conv layer followed by two sibling 1 × 1 conv\n",
    "layers (for reg and cls, respectively)**. ReLUs [15] are applied to the output of the n × n conv layer.\n",
    "\n",
    "\n",
    "At each sliding-window location(at each anchor point), we simultaneously predict k region proposals, so the reg layer has **4k outputs encoding the coordinates of k boxes**. The cls layer outputs **2k scores that estimate\n",
    "probability of object / not-object for each proposal**.2 The k proposals are parameterized relative to\n",
    "k reference boxes, called anchors. Each anchor is centered at the sliding window in question, and is\n",
    "associated with a scale and aspect ratio. We use 3 scales and 3 aspect ratios, yielding k = 9 anchors\n",
    "at each sliding position. For a conv feature map of a size W ×H (typically ∼2,400), there are W Hk\n",
    "anchors in total.\n",
    "\n",
    "\n",
    "## Training set labeling\n",
    "\n",
    "For training RPNs, we assign a binary class label (of being an object or not) to each anchor. We\n",
    "assign a positive label to two kinds of anchors: \n",
    "\n",
    "(i) the anchor/anchors with the highest Intersectionover-Union (IoU) overlap with a ground-truth box, \n",
    "\n",
    "(ii) an anchor that has an IoU overlap higher than 0.7 with any ground-truth box. \n",
    "\n",
    "Note that a single ground-truth box may assign positive labels\n",
    "to multiple anchors. \n",
    "\n",
    "We assign a negative label to a non-positive anchor if its IoU ratio is lower than 0.3 for all ground-truth boxes. Anchors that are neither positive nor negative do not contribute to the training objective.\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "L({pi}, {ti}) = 1\n",
    "Ncls\n",
    "X\n",
    "i\n",
    "Lcls (pi\n",
    ", p∗\n",
    "i\n",
    ") + λ\n",
    "1\n",
    "Nreg\n",
    "X\n",
    "i\n",
    "p\n",
    "∗\n",
    "i Lreg (ti\n",
    ", t∗\n",
    "i\n",
    ").\n",
    "\n",
    "Here, i is the index of an anchor in a mini-batch and pi\n",
    "is the predicted probability of anchor i being\n",
    "an object. The ground-truth label p\n",
    "∗\n",
    "i\n",
    "is 1 if the anchor is positive, and is 0 if the anchor is negative. ti\n",
    "is a vector representing the 4 parameterized coordinates of the predicted bounding box, and t\n",
    "∗\n",
    "i\n",
    "is that\n",
    "of the ground-truth box associated with a positive anchor. The classification loss Lcls is log loss over\n",
    "two classes (object vs. not object). For the regression loss, we use Lreg (ti\n",
    ", t∗\n",
    "i\n",
    ") = R(ti − t\n",
    "∗\n",
    "i\n",
    ") where\n",
    "R is the robust loss function (smooth L1) defined in [5]. The term p\n",
    "∗\n",
    "i Lreg means the regression loss\n",
    "is activated only for positive anchors (p\n",
    "∗\n",
    "i = 1) and is disabled otherwise (p\n",
    "∗\n",
    "i = 0). The outputs of\n",
    "the cls and reg layers consist of {pi} and {ti} respectively. The two terms are normalized with Ncls\n",
    "and Nreg , and a balancing weight λ.\n",
    "3\n",
    "\n",
    "\n",
    "\n",
    "For regression, we adopt the parameterizations of the 4 coordinates following [6]:\n",
    "\n",
    "tx = (x − xa)/wa,   ty = (y − ya)/ha,   tw = log(w/wa),   th = log(h/ha),\n",
    "t\n",
    "∗\n",
    "x = (x\n",
    "∗ − xa)/wa, t∗\n",
    "y = (y\n",
    "∗ − ya)/ha, t∗\n",
    "w = log(w\n",
    "∗\n",
    "/wa), t∗\n",
    "h = log(h\n",
    "∗\n",
    "/ha),\n",
    "\n",
    "where x, y, w, and h denote the two coordinates of the box center, width, and height. Variables\n",
    "x, xa, and x\n",
    "∗\n",
    "are for the predicted box, anchor box, and ground-truth box respectively (likewise\n",
    "for y, w, h). This can be thought of as bounding-box regression from an anchor box to a nearby\n",
    "ground-truth box.\n",
    "\n",
    "\n",
    "## Mini batch sampling and hyper parameters\n",
    "\n",
    "Each mini-batch arises from a single image\n",
    "that contains many positive and negative anchors. It is possible to optimize for the loss functions of\n",
    "all anchors, but this will bias towards negative samples as they are dominate. Instead, we randomly\n",
    "sample 256 anchors in an image to compute the loss function of a mini-batch, where the sampled\n",
    "positive and negative anchors have a ratio of up to 1:1. If there are fewer than 128 positive samples\n",
    "in an image, we pad the mini-batch with negative ones.\n",
    "\n",
    "We randomly initialize all new layers by drawing weights from a zero-mean Gaussian distribution\n",
    "with standard deviation 0.01. \n",
    "\n",
    "All other layers (i.e., the shared conv layers) are initialized by pretraining a model for ImageNet classification, as is standard practice. We tune all conv3 1 and up for the VGG net to conserve memory [5]. \n",
    "\n",
    "We use a **learning rate\n",
    "of 0.001** for 60k mini-batches, and 0.0001 for the next 20k mini-batches on the PASCAL dataset.\n",
    "We also use a momentum of 0.9 and a weight decay of 0.0005 [11].\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "It takes as input an conv feature map and a set\n",
    "of object proposals. For each object proposal from the RPN a region of interest (RoI) pooling layer extracts a fixed-length feature vector from the feature map.\n",
    "\n",
    "\n",
    "Each feature vector is fed into a sequence of fully connected\n",
    "(fc) layers that finally branch into two sibling output layers: \n",
    "\n",
    "one that produces softmax probability estimates over K object classes plus a catch-all “background” class and\n",
    "\n",
    "\n",
    "another layer that outputs four real-valued numbers for each\n",
    "of the K object classes. Each set of 4 values encodes refined\n",
    "bounding-box positions for one of the K classes.\n",
    "\n",
    "\n",
    "## Training set labeling\n",
    "\n",
    " As\n",
    "in [9], we take 25% of the RoIs from object proposals that\n",
    "have intersection over union (IoU) overlap with a groundtruth bounding box of at least 0.5. These RoIs comprise\n",
    "the examples labeled with a foreground object class, i.e.\n",
    "u ≥ 1. The remaining RoIs are sampled from object proposals that have a maximum IoU with ground truth in the interval , following . These are the background\n",
    "examples and are labeled with u = 0.\n",
    "\n",
    "## Loss function\n",
    "\n",
    "\n",
    "Each training RoI is labeled with a ground-truth class u\n",
    "and a ground-truth bounding-box regression target v. \n",
    "\n",
    "We\n",
    "use a multi-task loss L on each labeled RoI to jointly train\n",
    "for classification and bounding-box regression:\n",
    "\n",
    "L(p, u, tu\n",
    ", v) = Lcls(p, u) + λ[u ≥ 1]Lloc(t\n",
    "u\n",
    ", v), \n",
    "\n",
    "\n",
    "in which Lcls(p, u) = − log pu is log loss for true class u.\n",
    "The second task loss, Lloc, is defined over a tuple of\n",
    "true bounding-box regression targets for class u, v =\n",
    "(vx, vy, vw, vh), and a predicted tuple t\n",
    "u = (t\n",
    "u\n",
    "x\n",
    ", tu\n",
    "y\n",
    ", tu\n",
    "w, tu\n",
    "h\n",
    "),\n",
    "again for class u\n",
    "\n",
    "## Mini batch sampling and hyper parameters \n",
    "\n",
    "  During fine-tuning, each SGD\n",
    "mini-batch is constructed from N = 2 images, chosen uniformly at random (as is common practice, we actually iterate over permutations of the dataset). We use mini-batches\n",
    "of size R = 128, sampling 64 RoIs from each image. The sampling starategy is described in the labeling part\n",
    "\n",
    "The fully connected layers used for softmax classification and bounding-box regression are\n",
    "initialized from zero-mean Gaussian distributions with standard deviations 0.01 and 0.001, respectively. Biases are initialized to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposal flow from RPN To detection network\n",
    "\n",
    " During training, we\n",
    "ignore all cross-boundary anchors so they do not contribute to the loss. For a typical 1000 × 600\n",
    "image, there will be roughly 20k (≈ 60 × 40 × 9) anchors in total. With the cross-boundary anchors\n",
    "ignored, there are about 6k anchors per image for training.\n",
    "\n",
    "During testing, however, we still apply the fully-convolutional RPN to the entire\n",
    "image. This may generate cross-boundary proposal boxes, which we clip to the image boundary.\n",
    "\n",
    "Some RPN proposals highly overlap with each other. To reduce redundancy (during training and testing), we adopt nonmaximum suppression (NMS) on the proposal regions based on their cls scores. We fix the IoU\n",
    "threshold for NMS at 0.7, which leaves us about 2k proposal regions per image.\n",
    "\n",
    "After NMS, we use the top-N ranked proposal regions for detection(during testing most probably). \n",
    "\n",
    "We\n",
    "train Fast R-CNN using 2k RPN proposals, but evaluate different numbers of proposals at test-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing procedure\n",
    "\n",
    "\n",
    "At test time, the 20k anchors from each image go through a series of post-processing steps to send in the object proposal bounding boxes.\n",
    "\n",
    "The regression coefficients are applied to the anchors for precise localization. This gives precise bounding boxes.\n",
    "\n",
    "All the boxes are arranged according to their cls scores. Then, a non-maximum suppression (NMS) is applied with a threshold of 0.7. From the top down, all of the bounding boxes which have an IoU of greater than 0.7 with another bounding box are discarded. Thus the highest-scoring bounding box is retained for a group of overlapping boxes.\n",
    "This gives about 2k proposals per image.\n",
    "\n",
    "The cross-boundary bounding boxes are retained and clipped to image boundary.At train time, they are ignored \n",
    "\n",
    "While using these object proposals to train the Fast R-CNN detection pipeline, all 2k proposals from the RPN are used. At test time for Fast R-CNN detection, only the Top N proposals from the RPN are chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Four-step training\n",
    "\n",
    "a)The RPN is trained independently as described above. The backbone CNN for this task is initialized with weights from a network trained for an ImageNet classification task, and is then fine-tuned for the region proposal task.\n",
    "\n",
    "b) The Fast R-CNN detector network is also trained independently. The backbone CNN for this task is initialized with weights from a network trained for an ImageNet classification task, and is then fine-tuned for the object detection task. The RPN weights are fixed and the proposals from the RPN are used to train the Faster R-CNN.\n",
    "\n",
    "c) The RPN is now initialized with weights from this Faster R-CNN, and fine-tuned for the region proposal task. This time, weights in the common layers between the RPN and detector remain fixed, and only the layers unique to the RPN are fine-tuned. This is the final RPN.\n",
    "\n",
    "d) Once again using the new RPN, the Fast R-CNN detector is fine-tuned. Again, only the layers unique to the detector network are fine-tuned and the common layer weights are fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good reading material on Faster R-CNN\n",
    "\n",
    "* https://towardsdatascience.com/faster-r-cnn-for-object-detection-a-technical-summary-474c5b857b46\n",
    "* https://dongjk.github.io/code/object+detection/keras/2018/06/10/Faster_R-CNN_step_by_step,_Part_II.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
