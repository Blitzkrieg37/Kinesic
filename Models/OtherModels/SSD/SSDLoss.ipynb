{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSDLoss:\n",
    "    \n",
    "    def __init__(self,neg_pos_ratio=3, n_neg_min=0, alpha=1.0):\n",
    "        self.neg_pos_ratio = neg_pos_ratio\n",
    "        self.n_neg_min = n_neg_min\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def smooth_L1_loss(self, y_true, y_pred):\n",
    "        '''\n",
    "        Compute smooth L1 loss,\n",
    "        Arguments:\n",
    "            y_true (nD tensor): A TensorFlow tensor of any shape containing the ground truth data.\n",
    "                In this context, the expected tensor has shape `(batch_size, #boxes, 4)` and\n",
    "                contains the ground truth bounding box offsets, where the last dimension\n",
    "                contains offsets.\n",
    "            y_pred (nD tensor): A TensorFlow tensor of identical structure to `y_true` containing\n",
    "                the predicted data, in this context the predicted bounding box coordinates.\n",
    "        Returns:\n",
    "            The smooth L1 loss, a nD-1 Tensorflow tensor. In this context a 2D tensor\n",
    "            of shape (batch, n_boxes_total).\n",
    "        References:\n",
    "            https://arxiv.org/abs/1504.08083\n",
    "        '''\n",
    "        absolute_loss = tf.abs(y_true - y_pred)\n",
    "        square_loss = 0.5 * (y_true - y_pred)**2\n",
    "        l1_loss = tf.where(tf.less(absolute_loss, 1.0), square_loss, absolute_loss - 0.5)\n",
    "        return tf.reduce_sum(l1_loss, axis=-1)\n",
    "    \n",
    "    def log_loss(self, y_true, y_pred):\n",
    "        '''\n",
    "        Compute the softmax log loss.\n",
    "        Arguments:\n",
    "            y_true (nD tensor): A TensorFlow tensor of any shape containing the ground truth data.\n",
    "                In this context, the expected tensor has shape (batch_size, #boxes, #classes+1)(with background class)\n",
    "                and contains the ground truth bounding box categories.\n",
    "            y_pred (nD tensor): A TensorFlow tensor of identical structure to `y_true` containing\n",
    "                the predicted data, in this context the predicted bounding box categories.\n",
    "        Returns:\n",
    "            The softmax log loss, a nD-1 Tensorflow tensor. In this context a 2D tensor\n",
    "            of shape (batch, n_boxes_total).\n",
    "        '''\n",
    "        # Make sure that `y_pred` doesn't contain any zeros (which would break the log function)\n",
    "        y_pred = tf.maximum(y_pred, 1e-15)\n",
    "        # Compute the log loss\n",
    "        log_loss = -tf.reduce_sum(y_true * tf.log(y_pred), axis=-1)\n",
    "        return log_loss\n",
    "    \n",
    "    \n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        '''\n",
    "        Compute the loss of the SSD model prediction against the ground truth.\n",
    "        Arguments:\n",
    "            y_true (array): A Numpy array of shape `(batch_size, #boxes, #classes + 5)`,\n",
    "                where `#boxes` is the total number of boxes that the model predicts\n",
    "                per image. Be careful to make sure that the index of each given\n",
    "                box in `y_true` is the same as the index for the corresponding\n",
    "                box in `y_pred`. \n",
    "            y_pred (Keras tensor): The model prediction. The shape is identical\n",
    "                to that of `y_true`, i.e. `(batch_size, #boxes, #classes + 5)`.\n",
    "                The last axis must contain entries in the format\n",
    "                `[classes+1 one-hot encoded, 4 predicted box coordinate offsets]`.\n",
    "        Returns:\n",
    "            A scalar, the total multitask loss for classification and localization.\n",
    "        '''\n",
    "        self.neg_pos_ratio = tf.constant(self.neg_pos_ratio)\n",
    "        self.n_neg_min = tf.constant(self.n_neg_min)\n",
    "        self.alpha = tf.constant(self.alpha)\n",
    "\n",
    "        batch_size = tf.shape(y_pred)[0] \n",
    "        n_boxes = tf.shape(y_pred)[1] \n",
    "        \n",
    "        classification_loss = tf.to_float(self.log_loss(y_true[:,:,:-4], y_pred[:,:,:-4])) # Output shape: (batch_size, n_boxes)\n",
    "        localization_loss = tf.to_float(self.smooth_L1_loss(y_true[:,:,-4:], y_pred[:,:,-4:])) # Output shape: (batch_size, n_boxes)\n",
    "        \n",
    "        # masks\n",
    "        negatives = y_true[:,:,-5] # Tensor of shape (batch_size, n_boxes)\n",
    "        positives = tf.to_float(tf.reduce_max(y_true[:,:,0:-5], axis=-1)) # Tensor of shape (batch_size, n_boxes)\n",
    "        \n",
    "        n_positive = tf.reduce_sum(positives)\n",
    "        \n",
    "        pos_class_loss = tf.reduce_sum(classification_loss * positives, axis=-1) # Tensor of shape (batch_size,)\n",
    "        neg_class_loss_all = classification_loss * negatives # Tensor of shape (batch_size, n_boxes)\n",
    "        n_neg_losses = tf.count_nonzero(neg_class_loss_all, dtype=tf.int32)\n",
    "        \n",
    "        n_negative_keep = tf.minimum(tf.maximum(self.neg_pos_ratio * tf.to_int32(n_positive), self.n_neg_min), n_neg_losses)\n",
    "        \n",
    "        \n",
    "        # In the unlikely case when either (1) there are no negative ground truth boxes at all\n",
    "        # or (2) the classification loss for all negative boxes is zero, return zero as the `neg_class_loss`.\n",
    "        def f1():\n",
    "            return tf.zeros([batch_size])\n",
    "\n",
    "        def f2():\n",
    "            \n",
    "            neg_class_loss_all_1D = tf.reshape(neg_class_loss_all, [-1]) # Tensor of shape (batch_size * n_boxes,)\n",
    "            \n",
    "            # get the indices for the `n_negative_keep` boxes with the highest loss out of those...\n",
    "            values, indices = tf.nn.top_k(neg_class_loss_all_1D,\n",
    "                                          k=n_negative_keep,\n",
    "                                          sorted=False) \n",
    "            # with these indices create a mask...\n",
    "            negatives_keep = tf.scatter_nd(indices=tf.expand_dims(indices, axis=1),\n",
    "                                           updates=tf.ones_like(indices, dtype=tf.int32),\n",
    "                                           shape=tf.shape(neg_class_loss_all_1D)) # Tensor of shape (batch_size * n_boxes,)\n",
    "            negatives_keep = tf.to_float(tf.reshape(negatives_keep, [batch_size, n_boxes])) # Tensor of shape (batch_size, n_boxes)\n",
    "            # ...and use it to keep only those boxes and mask all other classification losses\n",
    "            neg_class_loss = tf.reduce_sum(classification_loss * negatives_keep, axis=-1) # Tensor of shape (batch_size,)\n",
    "            return neg_class_loss\n",
    "\n",
    "        neg_class_loss = tf.cond(tf.equal(n_neg_losses, tf.constant(0)), f1, f2)\n",
    "\n",
    "        class_loss = pos_class_loss + neg_class_loss # Tensor of shape (batch_size,)\n",
    "\n",
    "        # 3: Compute the localization loss for the positive targets.\n",
    "        #    We don't compute a localization loss for negative predicted boxes (obviously: there are no ground truth boxes they would correspond to).\n",
    "\n",
    "        loc_loss = tf.reduce_sum(localization_loss * positives, axis=-1) # Tensor of shape (batch_size,)\n",
    "\n",
    "        # 4: Compute the total loss.\n",
    "\n",
    "        total_loss = (class_loss + self.alpha * loc_loss) / tf.maximum(1.0, n_positive)\n",
    "        \n",
    "        # Keras divides the loss by the batch size        \n",
    "        total_loss = total_loss * tf.to_float(batch_size)\n",
    "        \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
